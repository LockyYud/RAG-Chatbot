{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dO1GLTnM9Mg"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEckkRd1M-LI",
        "outputId": "98fbcca6-9b55-4d85-b30f-c192f6d613ae"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain_community langchain_chroma langchain_experimental langchain-text-splitters\n",
        "%pip install -q langchain-groq langchain_openai\n",
        "%pip install -q langchain-huggingface\n",
        "%pip install -qU langchain-qdrant\n",
        "%pip install -qU qdrant_client\n",
        "%pip install -qU langchain-core\n",
        "%pip install -qU langchain-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIsc0yabNAfU",
        "outputId": "10422163-15b0-41c1-f85d-eb516b0ff7dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Any\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tiktoken import Encoding, encoding_for_model, get_encoding\n",
        "from statistics import mean\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
        "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
        "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otddDTwpNEIu"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJuGZNYNFto"
      },
      "outputs": [],
      "source": [
        "#LLMs and embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "llm_drafter = ChatOpenAI(model=\"gpt-4o-mini\", logprobs=True)\n",
        "llm_verifier = ChatOpenAI(model=\"gpt-4o\", logprobs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYHGFpp1Nmwr"
      },
      "outputs": [],
      "source": [
        "#Vector store\n",
        "qdrant_client = QdrantClient(\n",
        "    url=qdrant_url,\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Specify your collection name and query vector\n",
        "collection_name = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi perspective sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYJF0trOOUBe"
      },
      "outputs": [],
      "source": [
        "def multi_perspective_sampling(\n",
        "    k: int, retrieved_points: list[models.ScoredPoint], seed: int = 1399\n",
        ") -> list[list[str]]:\n",
        "    # Generate clusters\n",
        "    print(f\"Finding {k} clusters.\")\n",
        "    algo: Any = KMeans(n_clusters=k, random_state=seed)\n",
        "    _vectors = [point.vector for point in retrieved_points]\n",
        "    clusters: list[int] = algo.fit_predict(X=_vectors)\n",
        "\n",
        "    # Unique clusters\n",
        "    unique_clusters: set[int] = set(clusters)\n",
        "\n",
        "    # Create a dictionary with the members of each cluster\n",
        "    cluster_dict: defaultdict[int, list[int | None]] = defaultdict(list)\n",
        "    for index, cluster in enumerate(clusters):\n",
        "        cluster_dict[cluster].append(index)\n",
        "    print(f\"Clusters distribution: {dict(cluster_dict)}\")\n",
        "\n",
        "    # M subsets\n",
        "    m: int = min(len(indices) for indices in cluster_dict.values())\n",
        "    print(f\"{m} document subsets will be created.\")\n",
        "\n",
        "    np.random.seed(seed=seed)\n",
        "    subsets: list[list[str]] = []\n",
        "\n",
        "    for _ in range(m):\n",
        "        subset: list[int] = []\n",
        "        for cluster in unique_clusters:\n",
        "            chosen_element: int = np.random.choice(cluster_dict[cluster])\n",
        "            subset.append(chosen_element)\n",
        "            cluster_dict[cluster].remove(chosen_element)\n",
        "        subset_documents = [\n",
        "            retrieved_points[idx].payload.get(\"page_content\") for idx in subset\n",
        "        ]\n",
        "        subsets.append(subset_documents)\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG drafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AsyncOpenAI\n",
        "\n",
        "\n",
        "rag_drafting_prompt: str = \"\"\"Trả lời câu hỏi dựa trên các văn bản được cung cấp. Đồng thời cung cấp lý do đưa ra câu trả lời của bạn.\n",
        "## Câu hỏi: {query}\n",
        "\n",
        "## Văn bản cung cấp: {evidence}\"\"\"\n",
        "\n",
        "\n",
        "class RagDraftingResponse(BaseModel):\n",
        "    rationale: str = Field(description=\"Lý do đưa ra câu trả lời.\")\n",
        "    response: str = Field(description=\"Câu trả lời dựa trên các băn bản được cung cấp.\")\n",
        "\n",
        "\n",
        "async def rag_drafting_generator(\n",
        "    client: AsyncOpenAI,\n",
        "    model_name: str,\n",
        "    query: str,\n",
        "    evidence: str,\n",
        "    **kwargs,\n",
        ") -> tuple[RagDraftingResponse, float]:\n",
        "    completion: Any = await client.beta.chat.completions.parse(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": rag_drafting_prompt.format(\n",
        "                    instruction=query, evidence=evidence\n",
        "                ),\n",
        "            }\n",
        "        ],\n",
        "        response_format=RagDraftingResponse,\n",
        "        temperature=0.0,\n",
        "        logprobs=True,\n",
        "        max_tokens=512,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return (\n",
        "        completion.choices[0].message.parsed,\n",
        "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content)),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG verifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_verifier_prompt: str = \"\"\"## Câu hỏi: {query}\n",
        "\n",
        "## Câu trả lời: {response}\n",
        "\n",
        "## Lý do: {rationale}\n",
        "\n",
        "Lý do có đủ tốt để hỗ trợ cho câu trả lời không? (Có hoặc không)\"\"\"\n",
        "\n",
        "\n",
        "async def rag_verifier_generator(\n",
        "    client: AsyncOpenAI,\n",
        "    model_name: str,\n",
        "    instruction: str,\n",
        "    evidence: str,\n",
        "    response: str,\n",
        "    rationale: str,\n",
        "    **kwargs,\n",
        ") -> tuple[Any, float]:\n",
        "    encoder: Encoding = encoding_for_model(model_name=model_name)\n",
        "    completion: Any = await client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": rag_verifier_prompt.format(\n",
        "                    instruction=instruction,\n",
        "                    evidence=evidence,\n",
        "                    response=response,\n",
        "                    rationale=rationale,\n",
        "                ),\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        logprobs=True,\n",
        "        max_tokens=2,\n",
        "        **kwargs,\n",
        "    )\n",
        "    response: str = completion.choices[0].message.content\n",
        "    cond: bool = encoder.encode(text=response.lower()) == encoder.encode(text=\"yes\")\n",
        "    p_yes: float = (\n",
        "        np.exp(mean(token.logprob for token in completion.choices[0].logprobs.content))\n",
        "        if cond\n",
        "        else 0.0\n",
        "    )  # Naive\n",
        "\n",
        "    return (response, p_yes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End - to - end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxZ2u8sjRDoT"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from time import perf_counter\n",
        "from venv import logger\n",
        "\n",
        "from qdrant_client import AsyncQdrantClient\n",
        "\n",
        "async def speculative_rag(\n",
        "    query: str,\n",
        "    embedding_model: str,\n",
        "    collection_name: str,\n",
        "    k: int,\n",
        "    seed: int,\n",
        "    client: AsyncOpenAI,\n",
        "    qdrant_client: AsyncQdrantClient,\n",
        "    m_drafter: str,\n",
        "    m_verifier: str,\n",
        ") -> str:\n",
        "    _start = perf_counter()\n",
        "\n",
        "    # Generate query vector embedding\n",
        "    logger.info(\"Generating query vector...\")\n",
        "    _now: float = perf_counter()\n",
        "    query_vector: Any = await client.embeddings.create(\n",
        "        input=query, model=embedding_model\n",
        "    )\n",
        "    query_vector: list[float] = query_vector.data[0].embedding\n",
        "    logger.info(\"Query vector generated in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Fetching relevant documents\n",
        "    logger.info(\"Fetching relevant documents...\")\n",
        "    _now: float = perf_counter()\n",
        "    out: list[models.ScoredPoint] = await qdrant_client.search(\n",
        "        collection_name=collection_name, query_vector=query_vector, with_vectors=True\n",
        "    )\n",
        "    logger.info(\"Documents retrieved in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # Multi Perspective Sampling\n",
        "    logger.info(\"Doing Multi Perspective Sampling...\")\n",
        "    _now: float = perf_counter()\n",
        "    sampled_docs: list[list[str]] = multi_perspective_sampling(\n",
        "        k=k, retrieved_points=out, seed=seed\n",
        "    )\n",
        "    logger.info(\n",
        "        \"Multi Perspective Sampling done in {s:.4f} seconds.\", s=perf_counter() - _now\n",
        "    )\n",
        "\n",
        "    # RAG Drafting\n",
        "    logger.info(\"Doing RAG Drafting...\")\n",
        "    _now: float = perf_counter()\n",
        "    rag_drafts: list[tuple[RagDraftingResponse, float]] = await asyncio.gather(\n",
        "        *[\n",
        "            rag_drafting_generator(\n",
        "                client=client,\n",
        "                model_name=m_drafter,\n",
        "                query=query,\n",
        "                evidence=\"\\n\".join(\n",
        "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "                ),\n",
        "            )\n",
        "            for subset in sampled_docs\n",
        "        ]\n",
        "    )\n",
        "    logger.info(\"RAG Drafting done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    # RAG Verifier\n",
        "    logger.info(\"Doing RAG Verification...\")\n",
        "    _now: float = perf_counter()\n",
        "    rag_verifications: list[tuple[str, float]] = await asyncio.gather(\n",
        "        *[\n",
        "            rag_verifier_generator(\n",
        "                client=client,\n",
        "                model_name=m_verifier,\n",
        "                instruction=query,\n",
        "                evidence=\"\\n\".join(\n",
        "                    [f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)]\n",
        "                ),\n",
        "                response=rag_drafting_response.response,\n",
        "                rationale=rag_drafting_response.rationale,\n",
        "            )\n",
        "            for subset, (rag_drafting_response, _) in zip(sampled_docs, rag_drafts)\n",
        "        ]\n",
        "    )\n",
        "    logger.info(\"RAG Verification done in {s:.4f} seconds.\", s=perf_counter() - _now)\n",
        "\n",
        "    best_answer: int = np.argmax(\n",
        "        p_draft * p_self\n",
        "        for (_, p_draft), (_, p_self) in zip(rag_drafts, rag_verifications)\n",
        "    )\n",
        "    logger.info(\"Entire process done in {s:.4f} seconds.\", s=perf_counter() - _start)\n",
        "    print(f\"\\nQuestion:\\n ------ \\n{query}\\n\\n\")\n",
        "    print(f\"Response:\\n ------ \\n{rag_drafts[best_answer][0].response}\")\n",
        "    return rag_drafts[best_answer][0].response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spcr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
